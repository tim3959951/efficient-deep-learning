# âš¡ Efficient Deep Learning: Optimizing Energy Consumption in AI Training

This project explores **energy-efficient deep learning** by optimizing **training cost, accuracy, and computational resources**. Using **ResNet architectures, Xavier initialization, and pruning techniques**, we investigate how model depth, width, and learning rate impact **cumulative energy consumption** while maintaining high accuracy.

---

## ğŸš€ Project Overview

- **Objective**: Reduce energy consumption in deep learning training without compromising accuracy.
- **Model**: ResNet with **shortcut connections** & **Xavier initialization**
- **Techniques**: Pruning, Learning Rate Optimization, Width/Depth Trade-off Analysis
- **Key Findings**:
  - **Pruning + ResNet + Xavier reduces compute by 15-30%**
  - **Optimal learning rate (0.001) improves efficiency**
  - **Shortcut connections accelerate convergence**
- **Potential Applications**: **Edge AI, IoT, and low-power AI systems**

---

## ğŸ“‚ Project Structure

| File/Folder            | Description                                  |
|------------------------|----------------------------------------------|
| ğŸ“‚ src                | Core scripts for training and evaluation     |
| â”œâ”€â”€ train.py          | Train ResNet with Xavier initialization      |
| â”œâ”€â”€ evaluate.py       | Evaluate model accuracy & energy consumption |
| â”œâ”€â”€ energy_estimation.py | Compute cumulative energy consumption  |
| â”œâ”€â”€ prune.py         | Pruning techniques to reduce computation     |
| â”œâ”€â”€ data_processing.py | Normalize & standardize dataset           |
| â”œâ”€â”€ plot_results.py   | Generate accuracy-energy tradeoff plots     |
| ğŸ“‚ experiments                 | Model experiments & evaluation |
| ğŸ“‚ visualizations     | Stores training plots & energy consumption charts |
| ğŸ“„ ResNet_Energy_Efficiency_Pruning.ipynb | Full training pipeline notebook |
| ğŸ“„ requirements.txt   | Python dependencies                         |
| ğŸ“„ README.md          | Project documentation                       |

---

## ğŸ”¬ Data Processing & Normalization

In this project, **data preprocessing** is optimized for **energy-efficient model training**.

### **âœ… Normalization **
- **Image Pixel Scaling**:  
  - **[-1,1] Scaling**: Applied to image datasets to improve network stability.
  - Formula:  
    \[
    X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
    \]
  - Implemented in [`data_processing.py`](src/data_processing .py).

### **âœ… Standardization **
- **Zero Mean, Unit Variance Transformation**:  
  - Applied to structured input features to ensure stable gradient updates.
  - Formula:  
    \[
    X' = \frac{X - \mu}{\sigma}
    \]
  - Used for non-image datasets.

### **âœ… Energy Consumption Data Handling**
- Computed per **input-hidden and hidden-output layers** to track efficiency.
- Accumulated energy usage analyzed across **training epochs**.

ğŸš€ **Implemented in**: [`energy_estimation.py`](src/energy_estimation.py)

---

## ğŸ— Model Architecture

The **ResNet-based model** is optimized for efficient deep learning training:

```python
model = ResNet(depth=34, shortcut=True, initialization='xavier')
```

### **Why ResNet?**
âœ… **Shortcut connections** reduce gradient vanishing  
âœ… **Xavier initialization** improves weight scaling  
âœ… **Pruning techniques** remove redundant computations  

---

## **ğŸ”¥ FFNN vs. ResNet Energy Consumption**
Our study compared **Feedforward Neural Networks (FFNN)** and **ResNet** in terms of **computational cost**.

| Model         | Energy Efficiency | Computational Cost |
|--------------|-----------------|-------------------|
| **FFNN (3-layer)** | âŒ High energy   | ğŸ”º High FLOPs     |
| **ResNet (No Pruning)** | âš ï¸ Medium energy | ğŸ”¹ Moderate FLOPs |
| **ResNet + Pruning** | âœ… Low energy   | ğŸ”» Low FLOPs      |

ğŸš€ **Implemented in**: [`train.py`](src/train.py)  

---

## ğŸ¯ Training Strategy

ğŸš€ **Key Optimizations**  
- **Optimizer**: **Adam** (efficient gradient updates)  
- **Loss Function**: **Softmax Cross-Entropy** (for multi-class classification)  
- **Training Setup**:  
  - **50 epochs**, batch size = **64**  
  - **Pruning reduces compute by 30%**  
  - **Learning rate decay** applied for better convergence  

ğŸ”¹ **Energy-Aware Pruning Strategy**  
- Pruning was applied after every **5 epochs** to gradually reduce unnecessary parameters.  
- Models trained with pruning maintained **~98% accuracy** while using **30% fewer FLOPs**.  

ğŸš€ **Implemented in**: [`prune.py`](src/prune.py)  

---

## ğŸ“‰ Training Performance  

| **Accuracy Over Time** | **Energy Consumption Over Depth** |
|------------------|-----------------------------|
| ![Accuracy](visualizations/varydepthaccuracy_over_epochs.png) | ![Energy](visualizations/varydepth_energy_vs_accuracy.png) |

---

## ğŸ“ˆ Model Performance & Energy Efficiency  

âœ… **Confusion Matrix**: Evaluates model classification performance  
âœ… **Energy-Accuracy Tradeoff**: Visualizes computational efficiency  
âœ… **Depth vs. Energy Consumption**: Determines optimal architecture  

### **Energy vs Accuracy Tradeoff**
| **Varying Model Depth** | **ResNet Shortcut Impact** |
|-------------------|-------------------|
| ![Depth Impact](visualizations/resdepth.png) | ![Shortcut Efficiency](visualizations/resnet_shortcut.jpg) |

### **Epochs vs Accuracy Comparison**
| **Fixed Depth** | **Varying Depth** |
|-------------------|-------------------|
| ![Fixed Depth](visualizations/width_depth_acc.png) | ![Varying Depth](visualizations/width_depth_epoch.png) |

ğŸš€ **Implemented in**: [`evaluate.py`](src/evaluate.py)  

---

## ğŸ”¥ Key Takeaways  

ğŸ“Œ **Best Configuration**:  
- **ResNet + Xavier + Pruning** achieved **15-30% compute reduction** while maintaining accuracy.  
- **Optimal Learning Rate (0.001) improves convergence & efficiency**.  
- **ResNet Shortcut connections accelerate training and reduce energy cost**.  

ğŸ“Œ **Future Directions**:  
- ğŸ”¹ **Explore alternative architectures (e.g., EfficientNet, Hybrid CNN-RNNs) for better trade-offs**.  
- ğŸ”¹ **Investigate model quantization (TensorFlow Lite, ONNX) for real-time deployment**.  
- ğŸ”¹ **Test performance on edge devices (e.g., Raspberry Pi, Jetson Nano) for practical applications**.  

---

## ğŸŒ Why It Matters  

AI-driven deep learning models are becoming increasingly complex, leading to **high computational cost and energy consumption**. This research contributes to **Green AI**, focusing on:  

- ğŸŒ± **Reducing the carbon footprint of AI models**.  
- âš¡ **Improving energy efficiency for deep learning training**.  
- ğŸ“¶ **Enabling low-power AI applications in IoT & Edge AI**.  

ğŸš€ **This project paves the way for more sustainable AI solutions in real-world scenarios!**
